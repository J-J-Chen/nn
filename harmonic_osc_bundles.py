# -*- coding: utf-8 -*-
"""harmonic_osc_bundles.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dUwAyqzZLvY2AA3ddXV_gRzSy8-fqlJg

#### Constants, imports, GPU setup
"""

MODEL_PATH = "~/nn/"
FIGURE_PATH = "~/nn/"

import numpy as np
import torch
import torch.optim as optim
from torch.autograd import grad
import matplotlib.pyplot as plt
import math
import time
import copy
from os import path
from numpy.random import uniform
import sys

dtype = torch.float

"""#### Solve a simple first order ODE
$$ \ddot x(t) + \frac{kx}{m}=0 $$

#### Parametric, loss
"""

# Calculate the derivatice with auto-differention
def dfx(x,f):
    return grad([f], [x], grad_outputs=torch.ones(x.shape, dtype=dtype), create_graph=True)[0]

def perturbPoints(grid,t0,tf,sig=0.5):
#   stochastic perturbation of the evaluation points
#   force t[0]=t0  & force points to be in the t-interval
    delta_t = grid[1] - grid[0]  
    noise = delta_t * torch.randn_like(grid)*sig
    t = grid + noise
    t.data[2] = torch.ones(1,1)*(-1)
    t.data[t<t0]=t0 - t.data[t<t0]
    t.data[t>tf]=2*tf - t.data[t>tf]
    # t.data[0] = torch.ones(1,1)*t0
    t.requires_grad = False
    return t

# Only use x_hat, old parametric
def parametricSolutions(t, nn, initial_conditions):
    x_0, v_0, omega = initial_conditions[0][:], initial_conditions[1][:], initial_conditions[2][:]
    dt = t
    f = (1-torch.exp(-dt))

    t_bundle = torch.cat([t, x_0, v_0, omega], dim=1)

    N = forward(t_bundle, nn)

    return (x_0 + f * v_0 + f.pow(2) * N)

# Only one output, forward input bundle, x_N output
def forward(x, nn):
    y = nn.ffn(x)
    return y

def Eqs_Loss(t, x, omega):
    xdot = dfx(t,x)
    x2dot = dfx(t,xdot)
    f1 = x2dot + x*omega.pow(2)
    L = (f1.pow(2)).mean()
    return L

def old_Eqs_Loss(t,x1, X0):
    # Define the loss function by  Eqs.
    xdot = dfx(t,x1)
    x2dot = dfx(t,xdot)
    k = X0[2]
    m = X0[3]
    f1 = x2dot + (k*x1)/m
    L  = (f1.pow(2)).mean()
    return L

"""#### Network architecture (const)"""

# A two hidden layer NN, 1 input & 1 output
class odeNet(torch.nn.Module):
    def __init__(self, activation = None, input=4, layers=2, D_hid=128, output=1):
        super(odeNet,self).__init__()

        if activation is None:
          self.actF = torch.nn.Sigmoid()
        else:
          self.actF = activation

        self.fca = torch.nn.Sequential(
            torch.nn.Linear(D_hid, D_hid),
            self.actF
        )

        self.ffn = torch.nn.Sequential(
            torch.nn.Linear(input, D_hid),
            *[self.fca for _ in range(layers)],
            torch.nn.Linear(D_hid, output)
        )

    def forward(self,t):
        x = self.ffn(t)
        return x

"""#### odeNet"""

# Train the NN
def run_odeNet(initial_conditions_set, tf, neurons, epochs, n_train,lr, PATH= "", loadWeights=False,
                    minibatch_number = 1, minLoss=1e-3):
                    
    PATH = MODEL_PATH + PATH
    fc0 = odeNet(activation = None, input = 4, layers=5, output=1, D_hid=neurons)
    fc1 =  copy.deepcopy(fc0) # fc1 is a deepcopy of the network with the lowest training loss
    # optimizer
    betas = [0.999, 0.9999]
    
    optimizer = optim.Adam(fc0.parameters(), lr=lr, betas=betas)
    Loss_history = [];     Llim =  1 
        
    t0, initial_conditions_set = initial_conditions_set[0], initial_conditions_set[1:]
    grid = torch.linspace(t0.item(), tf, n_train).reshape(-1,1)
    
        
## LOADING WEIGHTS PART if PATH file exists and loadWeights=True
    if path.exists(PATH) and loadWeights==True:
        checkpoint = torch.load(PATH)
        fc0.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        tt = checkpoint['epoch']
        Ltot = checkpoint['loss']
        fc0.train(); # or model.eval
    
## TRAINING ITERATION    
    TeP0 = time.time()
    using_gpu = torch.cuda.is_available()
    if using_gpu:
        scaler = torch.cuda.amp.GradScaler()
    for tt in range(epochs):                
# Perturbing the evaluation points & forcing t[0]=t0
        # t=perturbPoints(grid,t0,tf,sig=.03*tf)
        t=perturbPoints(grid,t0,tf,sig= 0.3*tf)
            
# BATCHING
        batch_size = int(n_train/minibatch_number)
        batch_start, batch_end = 0, batch_size

        idx = np.random.permutation(n_train)
        t_b = t[idx]
        t_b.requires_grad = True

        loss=0.0
        for nbatch in range(minibatch_number): 
# batch time set
            t_mb = t_b[batch_start:batch_end]
#  Network solutions 
            x_0 = uniform(initial_conditions_set[0][0], initial_conditions_set[0][1], size=batch_size)
            v_0 = uniform(initial_conditions_set[1][0], initial_conditions_set[1][1], size=batch_size)
            omega = uniform(initial_conditions_set[2][0], initial_conditions_set[2][1], size=batch_size)
            x_0 = torch.Tensor([x_0]).reshape((-1,1))
            v_0 = torch.Tensor([v_0]).reshape((-1,1))
            omega = torch.Tensor([omega]).reshape((-1,1))
            initial_conditions = [x_0, v_0, omega]

            x = parametricSolutions(t_mb,fc0,initial_conditions)
            
# LOSS
#  Loss function defined by Hamilton Eqs. (symplectic): Writing explicitely the Eqs (faster)
            if using_gpu:
                with torch.cuda.amp.autocast():
                    Ltot = Eqs_Loss(t_mb,x, omega)
            else:
                Ltot = Eqs_Loss(t_mb,x, omega)

#  Loss function defined by Hamilton Eqs. (symplectic): Calculating with auto-diff the Eqs (slower)
#             Ltot = hamEqs_Loss_byH(t_mb,x,y,px,py,lam)
    

# OPTIMIZER
            if using_gpu:
                scaler.scale(Ltot).backward(retain_graph=False)
                scaler.step(optimizer)
                scaler.update()
            else:
                Ltot.backward(retain_graph=False); #True
                optimizer.step(); 
            loss += Ltot.detach()
       
            optimizer.zero_grad()

            batch_start +=batch_size
            batch_end +=batch_size

            if (tt+1)%100 == 0 and tt != 0:
                print(f'\rLoss: {loss}, Step: {tt}', end='')

# keep the loss function history
        Loss_history.append(loss)       

#Keep the best model (lowest loss) by using a deep copy
        if  tt > 0.8*epochs  and Ltot < Llim:
            fc1 =  copy.deepcopy(fc0)
            Llim=Ltot 

# break the training after a thresold of accuracy
        if Ltot < minLoss :
            fc1 =  copy.deepcopy(fc0)
            print('Reach minimum requested loss')
            break

    TePf = time.time()
    runTime = TePf - TeP0     
    
    
    torch.save({
    'epoch': tt,
    'model_state_dict': fc1.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': Ltot,
    }, PATH)

    return fc1, Loss_history, runTime

def trainModel(initial_conditions_set, t_max, neurons, epochs, n_train, lr,  loadWeights=True, minLoss=1e-6, showLoss=True, PATH =""):
    model,loss,runTime = run_odeNet(initial_conditions_set, t_max, neurons, epochs, n_train,lr, PATH,  loadWeights=loadWeights, minLoss=minLoss, minibatch_number=1)

    np.savetxt('loss.txt',loss)
    
    if showLoss==True :
        print('Training time (minutes):', runTime/60)
        print('Training Loss: ',  loss[-1] )
        plt.figure()
        plt.loglog(loss,'-b',alpha=0.975);                
        plt.tight_layout()
        plt.ylabel('Loss');plt.xlabel('t')
    
        # plt.savefig('HHsystem/HenonHeiles_loss.png')
        plt.savefig('simple_expDE_loss.png')
    

def loadModel(PATH):
    PATH = MODEL_PATH + PATH
    if path.exists(PATH):
        fc0 = odeNet(activation = None, input = 4, layers=5, output=1, D_hid=neurons)
        checkpoint = torch.load(PATH)
        fc0.load_state_dict(checkpoint['model_state_dict'])
        fc0.train(); # or model.eval
    else:
        print('Warning: There is not any trained model. Terminate')
        sys.exit()

    return fc0

"""#### Small omega"""

# TRAIN THE NETWORK. 

# Set the time range and the training points N
t0, t_max, N = 0.,  2*np.pi, 500;     
# Set the initial state. lam controls the nonlinearity
x0 = 4
#k = 1
#m = 1
v0=0
x0_set = [-2,2]
v0_set = [0,2]
omega_set = [0.5,0.8]
initial_conditions_set = []
initial_conditions_set.append(torch.Tensor([t0]).reshape(-1,1))
initial_conditions_set.append(x0_set)
initial_conditions_set.append(v0_set)
initial_conditions_set.append(omega_set)
#X0 = [t0, x0,k,m,v0]

# Here, we use one mini-batch. NO significant different in using more
n_train, neurons, epochs, lr = N, 128, int( 1e3 ), 8e-3
trainModel(initial_conditions_set, t_max, neurons, epochs, n_train, lr, PATH="freq",  loadWeights=False, minLoss=1e-6, showLoss=True)
model = loadModel("freq")

"""Test the predictions"""

nTest = N ; t_max_test = 1.0*t_max
tTest = torch.linspace(t0,t_max_test,nTest)

tTest = tTest.reshape(-1,1);

initial_x = 1.5*torch.ones_like(tTest)
initial_v = 0*torch.ones_like(tTest)
omega = 0.7*torch.ones_like(tTest)

tTest.requires_grad=True
t_net = tTest.cpu().detach().numpy()

xTest = parametricSolutions(tTest,model,[initial_x, initial_v, omega])
xdotTest=dfx(tTest,xTest)

xTest=xTest.cpu().data.numpy()
xdotTest=xdotTest.cpu().data.numpy()

"""Ground Truth solution"""

phase_angle = 0
x_0=1.5
omega=0.7
x_exact = (x_0/np.cos(phase_angle))*np.cos(omega*t_net+phase_angle)
xdot_exact = -(x_0/np.cos(phase_angle))*(omega)*np.sin(omega*t_net+phase_angle)
#x_exact = (x_0/np.cos(phase_angle))*np.cos((2*math.pi*0.6)*t_net+phase_angle)
#xdot_exact = -(x_0/np.cos(phase_angle))*(1.2*math.pi)*np.sin((2*math.pi*0.6)*t_net+phase_angle)

"""Make some plots"""

################
# Make the plots
#################

lineW = 4 # Line thickness
plt.figure(figsize=(10,8))
plt.subplot(2,2,1)
plt.plot(t_net, x_exact,'-g', label='Ground Truth', linewidth=lineW);
plt.plot(t_net, xTest,'--b', label='x',linewidth=lineW, alpha=.5); 
plt.ylabel('x(t)');plt.xlabel('t')
plt.legend()


plt.subplot(2,2,2)
plt.plot(t_net, xdot_exact,'-g', label='Ground Truth', linewidth=lineW);
plt.plot(t_net, xdotTest,'--b', label='x',linewidth=lineW, alpha=.5); 
plt.ylabel('dx\dt');plt.xlabel('t')
plt.legend()

#plot xTest, y
#plt.figure(figsize=(10,8))
#plt.plot(xTest, xdotTest) 



# #plt.savefig('../results/HenonHeiles_trajectories.png')
plt.savefig(FIGURE_PATH+'simpleExp.png')
